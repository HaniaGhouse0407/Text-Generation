{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efb4ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dataset\n",
    "dataset_path = 'training.1600000.processed.noemoticon.csv'  # Replace with the actual path to the dataset\n",
    "df = pd.read_csv(dataset_path, encoding='latin-1', header=None)\n",
    "\n",
    "# Rename columns for better understanding\n",
    "df.columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "# Filter tweets from a specific user (if needed)\n",
    "user = 'BreannaBonana'  # Replace with the desired username\n",
    "tweets = df[df['user'] == user]['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f134417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hania\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hania\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Remove stopwords and perform stemming\n",
    "stopwords_english = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # Remove stopwords, punctuation, and convert to lowercase\n",
    "    tweet = tokenizer.tokenize(tweet)\n",
    "    tweet = [word for word in tweet if word not in stopwords_english and word not in string.punctuation]\n",
    "    # Perform stemming\n",
    "    tweet = [stemmer.stem(word) for word in tweet]\n",
    "    return tweet\n",
    "\n",
    "# Apply preprocessing to each tweet\n",
    "processed_tweets = [preprocess_tweet(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "861f3293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.8317\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.8201\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.8082\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.7956\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.7821\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.7673\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.7506\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.7317\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.7098\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.6843\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.6545\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.6195\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.5790\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.5330\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.4830\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.4322\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.3844\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.3391\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.2902\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.2331\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.1700\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.1077\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.0519\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.9995\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.9401\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.8709\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.8039\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.7481\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6891\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6234\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.5703\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.5130\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4502\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4019\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.3460\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.3078\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2576\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2201\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1764\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1365\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1046\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.0671\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0293\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9967\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9699\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.9520\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.9475\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.9470\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8721\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.9311\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "I cant cant sleep 1:30 1:30 1:30 1:30 1:30 1:30 1:30\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "\n",
    "# Create sequences from the processed tweets\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(processed_tweets)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "input_sequences = []\n",
    "for tweet in processed_tweets:\n",
    "    token_list = tokenizer.texts_to_sequences([tweet])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences for input into LSTM\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Split sequences into input (X) and output (y)\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "\n",
    "# Convert target output to one-hot encoded vectors\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "# Define and train the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(X, y, epochs=50, verbose=1)\n",
    "\n",
    "# Function to generate new text based on seed sentence\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict(token_list)[0]\n",
    "        predicted_word_index = np.argmax(predicted)\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_word_index:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "# Example usage:\n",
    "seed_sentence = \"I\"\n",
    "generated_tweet = generate_text(seed_sentence, 10, model, max_sequence_len)\n",
    "print(generated_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40e0fbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1100.5312320285234\n",
      "BLEU Score: 2.460081739093055e-78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hania\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\hania\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# Prepare a set of reference sentences for evaluation\n",
    "reference_sentences = [\"I love cats.\", \"This is a great day.\", \"The sky is blue.\"]\n",
    "\n",
    "# Generate a set of candidate sentences using the text generation algorithm\n",
    "candidate_sentences = [\"I love dogs.\", \"Today is a wonderful day.\", \"The sky is green.\"]\n",
    "\n",
    "# Calculate perplexity\n",
    "def calculate_perplexity(candidate_sentences, reference_sentences):\n",
    "    tokenized_references = [nltk.word_tokenize(sentence.lower()) for sentence in reference_sentences]\n",
    "    tokenized_candidates = [nltk.word_tokenize(sentence.lower()) for sentence in candidate_sentences]\n",
    "\n",
    "    all_tokens = [token for ref in tokenized_references for token in ref]\n",
    "    freq_dist = nltk.FreqDist(all_tokens)\n",
    "    total_words = len(all_tokens)\n",
    "\n",
    "    perplexities = []\n",
    "    for tokens in tokenized_candidates:\n",
    "        perplexity = 0\n",
    "        for token in tokens:\n",
    "            if freq_dist[token] > 0:\n",
    "                perplexity -= np.log(freq_dist[token] / total_words)\n",
    "            else:\n",
    "                perplexity -= np.log(1e-7 / total_words)  # Smoothing for unseen tokens\n",
    "        perplexity = np.exp(perplexity / len(tokens))\n",
    "        perplexities.append(perplexity)\n",
    "\n",
    "    return np.mean(perplexities)\n",
    "\n",
    "perplexity = calculate_perplexity(candidate_sentences, reference_sentences)\n",
    "print(\"Perplexity:\", perplexity)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate BLEU score\n",
    "def calculate_bleu(candidate_sentences, reference_sentences):\n",
    "    tokenized_references = [[nltk.word_tokenize(ref.lower())] for ref in reference_sentences]\n",
    "    tokenized_candidates = [nltk.word_tokenize(candidate.lower()) for candidate in candidate_sentences]\n",
    "\n",
    "    bleu_scores = []\n",
    "    for i in range(len(reference_sentences)):\n",
    "        reference = tokenized_references[i]\n",
    "        candidate = tokenized_candidates[i]\n",
    "        bleu_score = sentence_bleu(reference, candidate)\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "    return np.mean(bleu_scores)\n",
    "\n",
    "bleu_score = calculate_bleu(candidate_sentences, reference_sentences)\n",
    "print(\"BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dfb662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
